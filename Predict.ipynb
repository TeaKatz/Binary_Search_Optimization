{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRS = \"./Datasets/dataset_verysmall_balanced.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(DATASET_DIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRS = []\n",
    "for hp in range(1000, 10001, 100):\n",
    "    for df in range(10, 101, 10):\n",
    "        DATASET_DIRS.append(\"./Datasets/Dataset_verysmall_df10-100_hp1000-10000/dataset_100_df{}_hp{}.pkl\".format(df, hp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(DATASET_DIRS, list):\n",
    "    temp_loadeds = []\n",
    "    for dataset_dir in DATASET_DIRS:\n",
    "        temp_loadeds.append(pd.read_pickle(dataset_dir))\n",
    "    loaded = pd.concat(temp_loadeds, ignore_index=True)\n",
    "else:\n",
    "    loaded = pd.read_pickle(DATASET_DIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MONSTER_NUM = 1000\n",
    "MONSTER_HP_COLUMNS = [\"monster_hp_\" + str(num) for num in range(1, MAX_MONSTER_NUM + 1)]\n",
    "FEATURES = [\"focus_damage\", \"aoe_damage\", *MONSTER_HP_COLUMNS]\n",
    "TARGET = [\"attack_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_hps = np.zeros((loaded[\"monster_hps\"].shape[0], MAX_MONSTER_NUM), dtype=int)\n",
    "for row in range(len(loaded[\"monster_hps\"])):\n",
    "    # Sorted and padding\n",
    "    temp_hps[row, :len(loaded[\"monster_hps\"][row])] = sorted(loaded[\"monster_hps\"][row], reverse=True)\n",
    "    \n",
    "monster_hps = pd.DataFrame(temp_hps, columns=MONSTER_HP_COLUMNS)\n",
    "dataset = pd.concat([loaded, monster_hps], axis=1, ignore_index=False).drop(columns=\"monster_hps\")\n",
    "dataset.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset\n",
    "dataset.to_pickle(\"./Datasets/dataset_verysmall.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(dataset[TARGET].to_numpy().min(), dataset[TARGET].to_numpy().max(), 100, dtype=int)\n",
    "Y_bin = np.digitize(dataset[TARGET].to_numpy(), bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(dataset, random_state=42, shuffle=True, stratify=Y_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = train_set[FEATURES].to_numpy(), train_set[TARGET].to_numpy()\n",
    "X_test, Y_test = test_set[FEATURES].to_numpy(), test_set[TARGET].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = MinMaxScaler()\n",
    "X_train_scaled = X_scaler.fit_transform(X_train.astype(np.float32))\n",
    "\n",
    "Y_scaler = MinMaxScaler()\n",
    "Y_train_scaled = Y_scaler.fit_transform(Y_train.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./Checkpoints/X_scaler.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(Y_scaler, \"./Checkpoints/Y_scaler.pkl\")\n",
    "joblib.dump(X_scaler, \"./Checkpoints/X_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00031433]\n",
      "[0.00010478]\n"
     ]
    }
   ],
   "source": [
    "print(Y_scaler.min_)\n",
    "print(Y_scaler.scale_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    x = list(range(len(history.history[\"loss\"])))\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title(\"loss\")\n",
    "    plt.plot(x, loss)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.title(\"val_loss\")\n",
    "    plt.plot(x, val_loss)\n",
    "    \n",
    "def prediction_distribution(pred):\n",
    "    plt.hist(pred, bins=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [X_train.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDense(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense_1 = Dense(16, activation=\"relu\")\n",
    "        self.dense_2 = Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        output = self.dense_1(inputs)\n",
    "        output = self.dense_2(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class NonSequenceDense(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense_1 = Dense(16, activation=\"relu\")\n",
    "        self.dense_2 = Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        hps, damages = tf.split(inputs, [1000, 2], axis=1)\n",
    "        output = self.dense_1(hps)\n",
    "        output = tf.concat([output, damages], axis=1)\n",
    "        output = self.dense_2(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inbalance dataset is highly possible due to the following prediction distriburion.\n",
    "pred = model1.predict(X_train)\n",
    "prediction_distribution(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2 = SequenceDense()\n",
    "\n",
    "model2.compile(optimizer=Adam(learning_rate=0.0001), loss=\"mae\")\n",
    "history = model2.fit(X_train_scaled, Y_train_scaled, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inbalance dataset is highly possible due to the following prediction distriburion.\n",
    "pred = model2.predict(X_train_scaled)\n",
    "prediction_distribution(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save_weights(\"./Checkpoints/SequenceDenseBalanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3 = NonSequenceDense()\n",
    "\n",
    "model3.compile(optimizer=Adam(learning_rate=0.0001), loss=\"mae\")\n",
    "history = model3.fit(X_train_scaled, Y_train_scaled, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inbalance dataset is highly possible due to the following prediction distriburion.\n",
    "pred = model3.predict(X_train)\n",
    "prediction_distribution(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible cause of problem\n",
    "1. Inbalance dataset -> See prediction distribution -> Feed balance dataset\n",
    "2. Size of dataset -> Generate more data\n",
    "3. Scale of dataset -> Add Normalization\n",
    "4. Model's architecture -> try other architectures\n",
    "5. Size of model -> Increate model's size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inbalance dataset is highly possible due to the following prediction distriburion.\n",
    "pred = model.predict(X_train)\n",
    "prediction_distribution(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
