{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join, exists\n",
    "from os import makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"verysmall_balanced_cyclical\"\n",
    "wandb.init(project=\"binary_search_optimization\", name=run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"./datasets/dataset_verysmall_balanced.pkl\"\n",
    "WEIGHTS_DIR = join(\"./save_weights\", run_name)\n",
    "if not exists(WEIGHTS_DIR):\n",
    "    makedirs(WEIGHTS_DIR)\n",
    "\n",
    "MAX_MONSTER_NUM = 1000\n",
    "MONSTER_HPS_COL = [\"monster_hp_\" + str(num) for num in range(1, MAX_MONSTER_NUM + 1)]\n",
    "FEATURES_COL = [\"focus_damage\", \"aoe_damage\", *MONSTER_HPS_COL]\n",
    "TARGET_COL = [\"attack_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(DATASET_DIR)\n",
    "\n",
    "# Log data distribution\n",
    "for col in [\"focus_damage\", \"aoe_damage\", \"attack_num\"]:\n",
    "    plt.hist(dataset[col])\n",
    "    wandb.log({col: plt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(dataset[TARGET_COL].to_numpy().min(), dataset[TARGET_COL].to_numpy().max(), 100, dtype=int)\n",
    "Y_bin = np.digitize(dataset[TARGET_COL].to_numpy(), bins)\n",
    "\n",
    "try:\n",
    "    train_set, test_set = train_test_split(dataset, random_state=42, shuffle=True, stratify=Y_bin)\n",
    "except:\n",
    "    train_set, test_set = train_test_split(dataset, random_state=42, shuffle=True)\n",
    "\n",
    "X_train, Y_train = train_set[FEATURES_COL].to_numpy(), train_set[TARGET_COL].to_numpy()\n",
    "X_test, Y_test = test_set[FEATURES_COL].to_numpy(), test_set[TARGET_COL].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = MinMaxScaler() if not exists(join(WEIGHTS_DIR, \"X_scaler.pkl\")) else joblib.load(join(WEIGHTS_DIR, \"X_scaler.pkl\"))\n",
    "X_train_scaled = X_scaler.fit_transform(X_train.astype(np.float32))\n",
    "X_test_scaled = X_scaler.transform(X_test.astype(np.float32))\n",
    "\n",
    "Y_scaler = MinMaxScaler() if not exists(join(WEIGHTS_DIR, \"Y_scaler.pkl\")) else joblib.load(join(WEIGHTS_DIR, \"Y_scaler.pkl\"))\n",
    "Y_train_scaled = Y_scaler.fit_transform(Y_train.astype(np.float32))\n",
    "Y_test_scaled = Y_scaler.transform(Y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters for scalers\n",
    "joblib.dump(X_scaler, join(WEIGHTS_DIR, \"X_scaler.pkl\"))\n",
    "joblib.dump(Y_scaler, join(WEIGHTS_DIR, \"Y_scaler.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.losses import MAE\n",
    "from utilities import LearningRateFinder, Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.network_depth = 1\n",
    "wandb.config.network_width = 16\n",
    "wandb.config.activation = \"LeakyReLU\"\n",
    "wandb.config.optimizer = \"Adam\"\n",
    "wandb.config.loss = \"MAE\"\n",
    "wandb.config.stepsize = 20\n",
    "wandb.config.epochs = 160\n",
    "wandb.config.batch_size = 32\n",
    "wandb.config.validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDense(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = []\n",
    "        for _ in range(wandb.config.network_depth):\n",
    "            self.hidden_layers.append(Dense(wandb.config.network_width, activation=LeakyReLU()))\n",
    "        self.output_layer = Dense(1, activation=\"relu\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        output = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            output = layer(output)\n",
    "        output = self.output_layer(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model_{}_{}\".format(wandb.config.network_depth, wandb.config.network_width)\n",
    "\n",
    "model = SequenceDense()\n",
    "model.build(input_shape=X_train_scaled.shape)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"mae\")\n",
    "try:\n",
    "    model.load_weights(join(\"./save_weights\", model_name))\n",
    "except:\n",
    "    model.save_weights(join(\"./save_weights\", model_name))\n",
    "\n",
    "lr_finder = LearningRateFinder(model)\n",
    "lr_finder.find((X_train_scaled, Y_train_scaled), start_lr=1e-10, epochs=20)\n",
    "lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr_finder.losses)\n",
    "wandb.log({\"lr_finder\": plt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = lr_finder.lrs[15000]\n",
    "max_lr = lr_finder.lrs[25000]\n",
    "wandb.log({\"learning_rate_base\": base_lr})\n",
    "wandb.log({\"learning_rate_max\": max_lr})\n",
    "wandb.log({\"learning_rate_base_index\": 15000})\n",
    "wandb.log({\"learning_rate_max_index\": 25000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title(\"loss\")\n",
    "    plt.plot(loss)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.title(\"val_loss\")\n",
    "    plt.plot(val_loss)\n",
    "    \n",
    "class CLR(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.clr = Cosine(*args, **kwargs)\n",
    "        self.history = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        lr = self.clr()\n",
    "        self.history.append(lr)\n",
    "    \n",
    "        return lr\n",
    "    \n",
    "lr_updater = CLR(max_lr, base_lr, wandb.config.stepsize)\n",
    "\n",
    "model = SequenceDense()\n",
    "model.build(input_shape=X_train_scaled.shape)\n",
    "model.compile(optimizer=Adam(learning_rate=lr_finder.lrs[20000]), loss=\"mae\")\n",
    "model.load_weights(join(\"./save_weights\", model_name))\n",
    "history = model.fit(X_train_scaled, Y_train_scaled, \n",
    "                    epochs=wandb.config.epochs, \n",
    "                    batch_size=wandb.config.batch_size, \n",
    "                    validation_split=wandb.config.validation_split, \n",
    "                    verbose=1,\n",
    "                    callbacks=[lr_updater])\n",
    "model.save_weights(join(WEIGHTS_DIR, \"model_trained\", model_name))\n",
    "\n",
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "plt.title(\"train_loss\")\n",
    "plt.plot(loss)\n",
    "wandb.log({\"train_loss\": plt})\n",
    "\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "plt.title(\"val_loss\")\n",
    "plt.plot(loss)\n",
    "wandb.log({\"val_loss\": plt})\n",
    "\n",
    "test_loss = model.evaluate(X_test_scaled, Y_test_scaled, verbose=0)\n",
    "wandb.log({\"test_loss\": test_loss})\n",
    "\n",
    "lr_history = lr_updater.history\n",
    "plt.title(\"learning_rate_history\")\n",
    "plt.plot(lr_history)\n",
    "wandb.log({\"test_loss\": plt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_train_scaled)\n",
    "loss = MAE(Y_train_scaled, pred)\n",
    "plt.scatter(Y_train_scaled, loss)\n",
    "wandb.log({\"loss_dist\": wandb.Image(plt, caption=\"loss_dist\")})\n",
    "print(\"Average loss: {}\".format(np.mean(loss.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
